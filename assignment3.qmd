---
title: "Assignment 3"
author: "Anna Felip & Arnau Garcia"
format: pdf
editor: visual
---

**SOLUTION Problem 1, section (e):** We first introduce the data given in the table:

```{r}
x <-c(0.7,11.3,2.1,30.7,4.6,20.2,0.3,0.9,0.7,2.3,1.1,1.9,0.5,0.8,1.2,15.2,
      0.2,0.7,0.4,2.3)
y<-c(3.8, 4.6, 2.1,5.6,10.3,2.8,1.9,1.4,0.4,0.9,2.8,3.2,8.5,14.5,14.4,
     8.8,7.6,1.3,2.2,4.0)
n<-20
```

We try to plot the likelihood in order to get an intuition for a good initial value for the Fisher scoring method. First we program a function of the log-likelihood:

```{r}
loglike <- function(phi, x){
  return(length(x)*log(phi)-2*sum(log(1+phi*x)))
}
```

Now, using what we have seen in class for plotting likelihoods we can plot the likelihood:

```{r}
fred <- function(phi){
    return(apply(as.matrix(phi), 1,loglike, x = x/y))
}
curve(fred, from = min(x/y), to = max(x/y),
    xlab = expression(phi), ylab = expression(log(L(phi))))
```

It seems that, for instance, $\phi=1.5$ is a reasonable good starting value.

Function for compute the derivative of the log-likelihood:

```{r}
logder <- function(phi, x, n){
  return(n/phi - 2*sum(x/(1+phi*x)))
}
```

Function for compute the inverse of the expected Fisher information (we need the Fisher information for the whole sample):

```{r}
fisher_inv <- function(phi, n){
  return(3*phi^2/n)
}
```

Now, we program the Fisher scoring algorithm:

```{r}
tol<-10e-9
dif<-1
phi0<-1.5
iters<-100
count<-0
while(dif>tol & count<iters){
  phi <- phi0 + fisher_inv(phi0,n)*logder(phi0, x/y, n)
  dif <- abs(phi-phi0)
  phi0<-phi
  count<-count+1
}
cat("phi value:", phi, "iterations:", count)
```

Then, the Fisher Scoring Algorithm returns the MLE of $\phi$ as $\hat{\phi}_n=2.010169$. Now we check whether the output is a good result. We see if the equation that the MLE has to satisfy is satisfied:

```{r}
logder(phi,x/y,n)
```

Indeed, it is very near to 0. So, it seems that the result is good.

**SOLUTION Problem 3, section (b):**

We introduce the data:

```{r}
X<-c(4,2,6,8,3,5,7,1,9,10)
```

Setting the initial values given in the statements of the problem:

```{r}
lambda10 <- 3
lambda20 <- 7
pi0 <- 0.5
```

We program a function that computes the complete likelihood. We will use this problem after, for interpret the changing in each iteration of the EM algorithm in the likelihood value.

```{r}
likel <- function(x, l1, l2, p){
  return(prod(p*dpois(x,l1) + (1-p)*dpois(x,l2)))
}
```

Now, we program a function that does the EM algorithm.

```{r}
EM_TwoMixturePoisson <- function(pi0, lambda10, lambda20, X, iters=1000, tol=1e-3){
  dif <- 1
  count <-0
  #printing iteration 0
  cat("Iter", count, ": lambda1=", round(lambda10, 3), ", lambda2=",
        round(lambda20, 3), ", pi=", round(pi0, 3), 
        ", Likelihood value:",  likel(X, lambda10, lambda20, pi0), "\n")
  while(dif>tol & count<iters){
    #E-step
    f1 <- dpois(X, lambda10)
    f2 <- dpois(X, lambda20)
    gamma <- f1*pi0/(f1*pi0+f2*(1-pi0))
    #M-step:
    pi <- mean(gamma)
    lambda1 <- sum(gamma*X)/sum(gamma)
    lambda2 <- sum((1-gamma)*X)/sum(1-gamma)
    #chechking convergence (we use norm 2)
    dif <- sqrt((lambda1-lambda10)^2 + (lambda2-lambda20)^2)
    #update
    pi0 <- pi
    lambda10 <- lambda1
    lambda20 <- lambda2
    count <- count +1
    #printing results for each iteration
    cat("It", count, ": l1=", round(lambda1, 3), ", l2=",
        round(lambda2, 3), ", pi=", round(pi, 3), ", dif=", dif, 
        ", Likel val:",  likel(X, lambda1, lambda2, pi), "\n")
  }
}
```

We use the function in our case. Notice that the statements only demand the repetition of the E and M steps for a total of 3 iterations. Then, using the function we have:

```{r}
EM_TwoMixturePoisson(pi0, lambda10, lambda20, X, iters=3)
```

As we can see, in each iteration the value of $\pi$ is decreasing. We can observe that $\lambda_1,\lambda_2$ increase in the first iteration, but then decrease in the following iterations. The likelihood value is increasing, which is what we expect. The estimations obtained after three iterations are $\lambda_1=2.977$, $\lambda_2=7.124$ and $\pi=0.392$.

Now, we run the function using as a maximum iteration number arguments the one by default in the function ($1000$), and we see if the algorithm converges.

```{r}
EM_TwoMixturePoisson(pi0, lambda10, lambda20, X)
```

We can observe that the algorithm has converged after $34$ iterations, and the estimations are $\lambda_1=2.465$, $\lambda_2=6.767$ and $\pi=0.295$.
