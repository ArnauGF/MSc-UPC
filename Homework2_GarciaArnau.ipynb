{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arnau García Fernandez\n",
    "\n",
    "**SPDB: Homework II**\n",
    "\n",
    "Grab 2 different Datasets (or tables). One will be for predicting a continuous variable and the other will be for the categorical variable prediction. You can chose one from class but it is mandatory to have at least one of the datasets that is not from class: it will be more interesting for me if you look for something a little bit different (but easy enough to understand!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find a dataset and load it: You gotta start somewhere...\n",
    "2. Do an exploratory analysis: Check the variables, their meaning, some statistics... and obvi-\n",
    "ously separate train and test! Are there missing values? Need to treat them!\n",
    "3. Create the needed variables and modify the table as you need: If you discover that you need\n",
    "to do any changes, prepare the table so and get it ready to train!\n",
    "4. Train different models: train different models with the methods you find more useful (and\n",
    "maybe some that are not useful but will be nice to see why they are not: Regression, trees\n",
    "-GDT and RF(?)-, naivebayes... you can innovate a bit if you feel like it.\n",
    "5. Study the results of the model: make the proper analysis on the behaviour and predictions\n",
    "of the model.\n",
    "6. Discussion: Here, you would then have to chose which model works best and actually im-\n",
    "plement it into a certain problem. Just argue which one (for both examples) you would use\n",
    "and why (and why not the others?).\n",
    "You have until January 21st 2024 to deliver the work.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variable prediction\n",
    "We first start doing prediction of a categorical variable. We will be using a penguins data set from the public git hub repository of Victor Peña (teacher of the subject of Linear and generalised linear models of this master). In this data set we have the species of the penguins, the island where the penguins lives, the bill length (in mm), the bill depth (in mm), the flipper length (in mm), the body mass (in g), the sex and the year. We can find this data set in the following link (https://github.com/VicPena/VicPena.github.io/blob/master/glm/datasets/penguins_test.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all the stuff that we will be using during the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import spark.implicits._\n",
       "import org.apache.spark.ml.regression.{GeneralizedLinearRegression, GeneralizedLinearRegressionModel}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.feature.StringIndexer\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import spark.implicits._\n",
    "import org.apache.spark.ml.regression.{ GeneralizedLinearRegression, GeneralizedLinearRegressionModel }\n",
    "import org.apache.spark.ml.{ Pipeline, PipelineModel }\n",
    "import org.apache.spark.ml.tuning.{ CrossValidator, ParamGridBuilder }\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myfile: org.apache.spark.rdd.RDD[String] = /Users/arnaugarcia/Desktop/Q1/bases de dades/zeppelin/data/penguins_test.csv MapPartitionsRDD[1] at textFile at <console>:43\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myfile = sc.textFile(\"/Users/arnaugarcia/Desktop/Q1/bases de dades/zeppelin/data/penguins_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[String] = Array(species,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex,year, Adelie,Torgersen,38.9,17.8,181,3625,female,2007, Adelie,Torgersen,36.6,17.8,185,3700,female,2007, Adelie,Torgersen,42.5,20.7,197,4500,male,2007, Adelie,Biscoe,37.8,18.3,174,3400,female,2007, Adelie,Biscoe,38.2,18.1,185,3950,male,2007, Adelie,Biscoe,40.5,17.9,187,3200,female,2007, Adelie,Dream,37.2,18.1,178,3900,male,2007, Adelie,Dream,39.5,17.8,188,3300,female,2007, Adelie,Dream,36.4,17,195,3325,female,2007)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what we saw in class we can import the csv file in a properly format and we can print the observations in cells, it is, with a table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "penguin: org.apache.spark.sql.DataFrame = [species: string, island: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var penguin = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"/Users/arnaugarcia/Desktop/Q1/bases de dades/zeppelin/data/penguins_test.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "| Adelie|Torgersen|          38.9|         17.8|              181|       3625|female|2007|\n",
      "| Adelie|Torgersen|          36.6|         17.8|              185|       3700|female|2007|\n",
      "| Adelie|Torgersen|          42.5|         20.7|              197|       4500|  male|2007|\n",
      "| Adelie|   Biscoe|          37.8|         18.3|              174|       3400|female|2007|\n",
      "| Adelie|   Biscoe|          38.2|         18.1|              185|       3950|  male|2007|\n",
      "| Adelie|   Biscoe|          40.5|         17.9|              187|       3200|female|2007|\n",
      "| Adelie|    Dream|          37.2|         18.1|              178|       3900|  male|2007|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguin.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will filter and eliminate one type of the species. In our dataset we have three penguin species: \"Adeelie\", \"Gentoo\" and \"Chinstrap\". But, after review the spark documentation (see the documentation in https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression) about the avilaible methods for classification, we only have the binomial distribution avilable. For classify three categories we need the multinomial regression. Then, to be able to work with the tools provided by spark we delete one type of the species (the Chinstrap penguin species) and we will be classifying between two type of penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "penguin: org.apache.spark.sql.DataFrame = [species: string, island: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguin = penguin.filter($\"species\"!==\"Chinstrap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do som exploratory analysis, computing some statistics in order to understand how our dataset behaves. In addition, we study if there are missing values. First of all, we expose a summary with different basic statistics of the variables of the penguins datase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+------------------+------------------+------------------+-----------------+------+------------------+\n",
      "|summary|species|   island|    bill_length_mm|     bill_depth_mm| flipper_length_mm|      body_mass_g|   sex|              year|\n",
      "+-------+-------+---------+------------------+------------------+------------------+-----------------+------+------------------+\n",
      "|  count|     80|       80|                80|                80|                80|               80|    80|                80|\n",
      "|   mean|   NULL|     NULL|42.596250000000005|          16.72625|          202.2375|        4302.8125|  NULL|          2008.125|\n",
      "| stddev|   NULL|     NULL|5.2968928974530565|1.8606344195026496|14.382869337505177|823.7475509462067|  NULL|0.8171422880914381|\n",
      "|    min| Adelie|   Biscoe|              34.5|              13.3|               174|             2900|female|              2007|\n",
      "|    max| Gentoo|Torgersen|              59.6|              20.7|               230|             6050|  male|              2009|\n",
      "+-------+-------+---------+------------------+------------------+------------------+-----------------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summaryStats: org.apache.spark.sql.DataFrame = [summary: string, species: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summaryStats = penguin.describe()\n",
    "summaryStats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we count distinct values for each variable. Indeed, we obtain two type of species, which is exaclty what we want for do the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|      2|     3|            68|           46|               39|         56|  2|   3|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguin.select(penguin.columns.map(species => countDistinct(col(species)).alias(species)): _*).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we study if we have a balanced data set. What we want to assess is if we have a similar number of Adelie penguins and Gentoo penguins. Indeed, in the following chunk we see that we have a similar number, and then the data set is balanced and we can fit a model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|species|count|\n",
      "+-------+-----+\n",
      "| Adelie|   44|\n",
      "| Gentoo|   36|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "frequencyDistribution: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [species: string, count: bigint]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frequencyDistribution = penguin.groupBy(\"species\").count().orderBy(desc(\"count\"))\n",
    "frequencyDistribution.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute correlations between several continous variables. The correlation between \"bill_length_mm\" and \"bill_depth_mm\" is $-0.49$, meaning that when one of the variable increases the other decreases. It has a lot of sense because we are comparing the bill length and thr depth. If a penguin has a larger bill, then the depth should be smaller, otherwise the penguin would have a huge bill! On the other hand, the correlation between \"body_mass_g\" and \"flipper_length_mm\" is $0.89$, which has sense again, because we are comparing the mass of the penguin with the flipper. And it is quite clear that, the heavier the penguin, the bigger the penguin and then bigger flipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.4873960332591598\n",
      "Correlation: 0.8865533365292193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correlation1: Double = -0.4873960332591598\n",
       "correlation2: Double = 0.8865533365292193\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val correlation1 = penguin.stat.corr(\"bill_length_mm\", \"bill_depth_mm\")\n",
    "val correlation2 = penguin.stat.corr(\"body_mass_g\", \"flipper_length_mm\")\n",
    "\n",
    "println(s\"Correlation: $correlation1\")\n",
    "println(s\"Correlation: $correlation2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check for missing values in all the variables of the data set. We can observe that we have no missing values. So we can keep with our study on the data set without concern on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "|      0|     0|             0|            0|                0|          0|  0|   0|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguin.select(penguin.columns.map(colName => sum(when(col(colName).isNull, 1).otherwise(0)).alias(colName)): _*).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an instance for splitting our data into test and training data subsets. We choose to use the $70\\%$ of the data for training and the $30\\%$ of the data for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([species: string, island: string ... 6 more fields], [species: string, island: string ... 6 more fields])\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = penguin.randomSplit(Array(0.7, 0.3), seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "| Adelie|Biscoe|          34.5|         18.1|              187|       2900|female|2008|\n",
      "| Adelie|Biscoe|          35.7|         16.9|              185|       3150|female|2008|\n",
      "| Adelie|Biscoe|          37.6|         19.1|              194|       3750|  male|2008|\n",
      "| Adelie|Biscoe|          37.7|         16.0|              183|       3075|female|2009|\n",
      "| Adelie|Biscoe|          38.2|         18.1|              185|       3950|  male|2007|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "penguin_train: org.apache.spark.sql.DataFrame = [species: string, island: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val penguin_train = splits(0).toDF\n",
    "penguin_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "| Adelie|Biscoe|          37.8|         18.3|              174|       3400|female|2007|\n",
      "| Adelie|Biscoe|          38.1|         17.0|              181|       3175|female|2009|\n",
      "| Adelie|Biscoe|          42.2|         19.5|              197|       4275|  male|2009|\n",
      "| Adelie|Biscoe|          42.7|         18.3|              196|       4075|  male|2009|\n",
      "| Adelie| Dream|          36.0|         17.1|              187|       3700|female|2009|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "penguin_test: org.apache.spark.sql.DataFrame = [species: string, island: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val penguin_test = splits(1).toDF\n",
    "penguin_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look into the documentation, we see that for doing binary classifcation we must transofrm the variable to classify to $0,1$. This can be done using the _stringIndexer_ method. What we will do in the following chunks is prepare the training dataset for be trained. It is, we will create a new variable called _label_ such that if the species of the penguin is \"Adelie\" then label is $0$. And, if the species is \"Gentoo\", then the label is $1$. Also, we will use the features engineering. And, to use these methods optimally, we will be using the useful _pipeline_, which allows us to use different methods in the same code line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "labelindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_fb1d9a03c303\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_1a455dfe192c, handleInvalid=error, numInputCols=5\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_5b90aeeb2226\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "val labelindexer = new StringIndexer()\n",
    "      .setInputCol(\"species\")\n",
    "      .setOutputCol(\"label\")\n",
    "val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\"))\n",
    "      .setOutputCol(\"features\")\n",
    "val pipeline = new Pipeline()\n",
    "      .setStages(Array(labelindexer, assembler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the training data set prepared for apply the generalised linear rergression is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|label|            features|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "| Adelie|Biscoe|          34.5|         18.1|              187|       2900|female|2008|  0.0|[34.5,18.1,187.0,...|\n",
      "| Adelie|Biscoe|          35.7|         16.9|              185|       3150|female|2008|  0.0|[35.7,16.9,185.0,...|\n",
      "| Adelie|Biscoe|          37.6|         19.1|              194|       3750|  male|2008|  0.0|[37.6,19.1,194.0,...|\n",
      "| Adelie|Biscoe|          37.7|         16.0|              183|       3075|female|2009|  0.0|[37.7,16.0,183.0,...|\n",
      "| Adelie|Biscoe|          38.2|         18.1|              185|       3950|  male|2007|  0.0|[38.2,18.1,185.0,...|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "traindF: org.apache.spark.sql.DataFrame = [species: string, island: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var traindF = pipeline.fit(penguin_train).transform(penguin_train)\n",
    "traindF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prepare in the same way the tets data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|label|            features|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "| Adelie|Biscoe|          37.8|         18.3|              174|       3400|female|2007|  0.0|[37.8,18.3,174.0,...|\n",
      "| Adelie|Biscoe|          38.1|         17.0|              181|       3175|female|2009|  0.0|[38.1,17.0,181.0,...|\n",
      "| Adelie|Biscoe|          42.2|         19.5|              197|       4275|  male|2009|  0.0|[42.2,19.5,197.0,...|\n",
      "| Adelie|Biscoe|          42.7|         18.3|              196|       4075|  male|2009|  0.0|[42.7,18.3,196.0,...|\n",
      "| Adelie| Dream|          36.0|         17.1|              187|       3700|female|2009|  0.0|[36.0,17.1,187.0,...|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testdf: org.apache.spark.sql.DataFrame = [species: string, island: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var testdf = pipeline.fit(penguin_test).transform(penguin_test)\n",
    "testdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an instance of the generalized linear regression method, using the binomial regression and the logit link funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glr: org.apache.spark.ml.regression.GeneralizedLinearRegression = glm_5215fff0aa03\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val glr = new GeneralizedLinearRegression()\n",
    "            .setFamily(\"binomial\") \n",
    "            .setLink(\"logit\")\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the data with the previous \"estimator\" created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glrModel: org.apache.spark.ml.regression.GeneralizedLinearRegressionModel = GeneralizedLinearRegressionModel: uid=glm_5215fff0aa03, family=binomial, link=logit, numFeatures=5\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val glrModel = glr.fit(traindF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our model trained. But now, what we will do is to use cross validation for assessing how good is our model classifying the penguins species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numFolds: Int = 10\n",
       "MaxIter: Seq[Int] = List(100)\n",
       "RegParam: Seq[Double] = List(0.01)\n",
       "Tol: Seq[Double] = List(1.0E-4)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numFolds = 10\n",
    "val MaxIter: Seq[Int] = Seq(100)\n",
    "val RegParam: Seq[Double] = Seq(0.01) \n",
    "val Tol: Seq[Double] = Seq(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tglm_5215fff0aa03-maxIter: 100,\n",
       "\tglm_5215fff0aa03-regParam: 0.01,\n",
       "\tglm_5215fff0aa03-tol: 1.0E-4\n",
       "})\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(glrModel.maxIter, MaxIter)\n",
    "      .addGrid(glrModel.regParam, RegParam)\n",
    "      .addGrid(glrModel.tol, Tol)\n",
    "      .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use again the pipeline, which is very useful for do our code in a single line. In this case we define a pipeline with the objects `labelindexer`, `assembler` and `glr`. And then, we will pass the initial train data set, and the 0,1 labels will be created, also the features engineering, and in adittion the model will be trained with the estimator created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline2: org.apache.spark.ml.Pipeline = pipeline_d184cb1e0a4a\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline2 = new Pipeline()\n",
    "      .setStages(Array(labelindexer, assembler, glr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the necessary objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{ BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel }\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.tuning.{ ParamGridBuilder, CrossValidator }\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the binary classificator (remember that we are doing a binary classification, species 0 of penguin or species 1 of penguin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = BinaryClassificationEvaluator: uid=binEval_2596200c0619, metricName=areaUnderROC, numBins=1000\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setRawPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the cross validator object with the different objects defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossval: org.apache.spark.ml.tuning.CrossValidator = cv_332298d102db\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossval = new CrossValidator()\n",
    "      .setEstimator(pipeline2)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(numFolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to fit the model using the initial train data set and the cross validator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_332298d102db, bestModel=pipeline_d184cb1e0a4a, numFolds=10\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cvModel = crossval.fit(penguin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do predictions with respect the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+----------+\n",
      "|species|island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|label|            features|prediction|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+----------+\n",
      "| Adelie|Biscoe|          37.8|         18.3|              174|       3400|female|2007|  0.0|[37.8,18.3,174.0,...|   1.0E-16|\n",
      "| Adelie|Biscoe|          38.1|         17.0|              181|       3175|female|2009|  0.0|[38.1,17.0,181.0,...|   1.0E-16|\n",
      "| Adelie|Biscoe|          42.2|         19.5|              197|       4275|  male|2009|  0.0|[42.2,19.5,197.0,...|   1.0E-16|\n",
      "| Adelie|Biscoe|          42.7|         18.3|              196|       4075|  male|2009|  0.0|[42.7,18.3,196.0,...|   1.0E-16|\n",
      "| Adelie| Dream|          36.0|         17.1|              187|       3700|female|2009|  0.0|[36.0,17.1,187.0,...|   1.0E-16|\n",
      "+-------+------+--------------+-------------+-----------------+-----------+------+----+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predDF: org.apache.spark.sql.DataFrame = [species: string, island: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var predDF = cvModel.transform(penguin_test)\n",
    "predDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in the predictions that the predicted values are not exaclty 0 or 1. This is because these are the probabilities returned by a logistic regression, thus we have to choose a threshold. \n",
    "\n",
    "In order to see more clear how good are the predictions, we select only the variables \"species\", \"label\" and \"predicted_label\". We print several observations of this data set filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+\n",
      "|species|label|     Predicted_label|\n",
      "+-------+-----+--------------------+\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|             1.0E-16|\n",
      "| Adelie|  0.0|9.109044006745272E-9|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "| Gentoo|  1.0|  0.9999999999999999|\n",
      "+-------+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [species: string, label: double ... 1 more field]\n",
       "resutDF: org.apache.spark.sql.DataFrame = [species: string, label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = predDF.select(\"species\", \"label\", \"prediction\")\n",
    "val resutDF = result.withColumnRenamed(\"prediction\", \"Predicted_label\")\n",
    "resutDF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the predictions obtained are good. Notwithstanding we can see that there are some observations not well predicted. Now we want to compute different metrics for assess the quality of the model. First, we should transform the predicted values into 0 or 1. For this, we take as a threshold $0.5$, because is the more natural one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------------+\n",
      "|species|label|Predicted_label|\n",
      "+-------+-----+---------------+\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Adelie|  0.0|            0.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "| Gentoo|  1.0|            1.0|\n",
      "+-------+-----+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "condition: org.apache.spark.sql.Column = (prediction < 0.5)\n",
       "transformation: org.apache.spark.sql.Column = CASE WHEN (prediction < 0.5) THEN 0.0 ELSE 1.0 END\n",
       "predDF: org.apache.spark.sql.DataFrame = [species: string, island: string ... 9 more fields]\n",
       "result: org.apache.spark.sql.DataFrame = [species: string, label: double ... 1 more field]\n",
       "resutDF: org.apache.spark.sql.DataFrame = [species: string, label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define the condition and transformation\n",
    "val condition = col(\"prediction\") < 0.5\n",
    "val transformation = when(condition, 0.0).otherwise(1.0)\n",
    "\n",
    "// Apply the transformation using withColumn\n",
    "predDF = predDF.withColumn(\"prediction\", transformation)\n",
    "\n",
    "val result = predDF.select(\"species\", \"label\", \"prediction\")\n",
    "val resutDF = result.withColumnRenamed(\"prediction\", \"Predicted_label\")\n",
    "resutDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tVSpDF: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "TC: Long = 22\n",
       "tp: Double = 0.6363636363636364\n",
       "tn: Double = 0.36363636363636365\n",
       "fp: Double = 0.0\n",
       "fn: Double = 0.0\n",
       "MCC: Double = 1.0\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tVSpDF = predDF.select(\"label\", \"prediction\") // True vs predicted labels\n",
    "val TC = predDF.count() //Total count\n",
    "\n",
    "val tp = tVSpDF.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() / TC.toDouble\n",
    "val tn = tVSpDF.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count() / TC.toDouble\n",
    "val fp = tVSpDF.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() / TC.toDouble\n",
    "val fn = tVSpDF.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() / TC.toDouble\n",
    "    \n",
    "val MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn)) // Calculating Matthews correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 63.63636363636363%\n",
      "False positive rate: 0.0%\n",
      "True negative rate: 36.36363636363637%\n",
      "False negative rate: 0.0%\n",
      "Matthews correlation coefficient: 1.0\n"
     ]
    }
   ],
   "source": [
    "println(\"True positive rate: \" + tp *100 + \"%\")\n",
    "println(\"False positive rate: \" + fp * 100 + \"%\")\n",
    "println(\"True negative rate: \" + tn * 100 + \"%\")\n",
    "println(\"False negative rate: \" + fn * 100 + \"%\")\n",
    "println(\"Matthews correlation coefficient: \" + MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to interpret the values obtained in the computed metrics. Regarding the Matthews Correlation Coefficient (MCC), we have searched information on the web (see https://en.wikipedia.org/wiki/Phi_coefficient). We have found that \"The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.\" We obtained a MCC of $1$, thus, MCC is indicating that our model is doing the classifications perfect! \n",
    "\n",
    "If we study the other metric obtained: we have obtained a false negative rate of $0.0\\%$, which means that no penguins were classified as species class $0$, it is as Adelie, when actually the species was $1$, it is Gentoo. In addition, we have obtained a false positive rate of $0\\%$, meaning that no penguins were classified as Gentoo when they are actually Adelie. These two metrics indicates how many penguins are not well classified, and we see that all penguins are well classified for our model.\n",
    "\n",
    "Now, we compute the accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 1.0\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predDF)\n",
    "println(\"Classification accuracy: \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have obtained a nice accuracy, our model is classifying perfect the penguins! \n",
    "\n",
    "We compute ohter metrics, we compute the area under the precision-recall curve and the area under the ROC curve. As our model is doing perfect the classification we expect a AUC of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2996] at map at <console>:54\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabels = predDF\n",
    "      .select(\"prediction\", \"label\")\n",
    "      .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n",
    "        .asInstanceOf[Double]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the precision-recall curve: 1.0\n",
      "Area under the receiver operating characteristic (ROC) curve: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@74edef6c\n",
       "areaUnderPR: Double = 1.0\n",
       "areaUnderROC: Double = 1.0\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val areaUnderPR = metrics.areaUnderPR\n",
    "println(\"Area under the precision-recall curve: \" + areaUnderPR)\n",
    "val areaUnderROC = metrics.areaUnderROC\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve: \" + areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we were expecting, as our model is doing perfect the classification we obtain area under PR of $1$ and the same for the area under the ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally fit another model, this time a Naive Bayes model, because is a model that I have never used before, and I think that is inetresting to fit this class of model and see how it behaves. In addition, I am interested in compare the results with the ones obtained with the binomial regression.\n",
    "\n",
    "We will be using the same scheme than the used previously for fitting the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, NaiveBayes, NaiveBayesModel}\n",
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_30ee377e5545\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_11822bc50bbc\n",
       "paramGridnv: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tnb_30ee377e5545-smoothing: 0.01\n",
       "}, {\n",
       "\tnb_30ee377e5545-smoothing: 1.0E-4\n",
       "}, {\n",
       "\tnb_30ee377e5545-smoothing: 1.0E-6\n",
       "}, {\n",
       "\tnb_30ee377e5545-smoothing: 1.0E-8\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = BinaryClassificationEvaluator: uid=binEval_8be04f12c147, metricName=areaUnderROC, numBins=1000\n",
       "crossval: org.apache.spark.ml.tuning.CrossValidator = cv_af3d8ee368e6\n",
       "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_af3d8ee368e6, b...\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{ BinaryLogisticRegressionSummary, NaiveBayes, NaiveBayesModel }\n",
    "\n",
    "val nb = new NaiveBayes()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setFeaturesCol(\"features\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(labelindexer, assembler, nb))\n",
    "\n",
    "val paramGridnv = new ParamGridBuilder()\n",
    "      .addGrid(nb.smoothing, Array(1e-2, 1e-4, 1e-6, 1e-8))\n",
    "      .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setRawPredictionCol(\"prediction\")\n",
    "    \n",
    "val crossval = new CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGridnv)\n",
    "      .setNumFolds(numFolds)\n",
    "\n",
    "val cvModel = crossval.fit(penguin_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+\n",
      "|label|Predicted_label|         probability|\n",
      "+-----+---------------+--------------------+\n",
      "|  0.0|            0.0|[1.0,5.2530344100...|\n",
      "|  0.0|            0.0|[1.0,1.7794999354...|\n",
      "|  0.0|            0.0|[0.80895914643810...|\n",
      "|  0.0|            0.0|[0.99999999968301...|\n",
      "|  0.0|            0.0|[1.0,8.9705880274...|\n",
      "+-----+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predDF: org.apache.spark.sql.DataFrame = [species: string, island: string ... 11 more fields]\n",
       "result: org.apache.spark.sql.DataFrame = [label: double, prediction: double ... 1 more field]\n",
       "resutDF: org.apache.spark.sql.DataFrame = [label: double, Predicted_label: double ... 1 more field]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predDF = cvModel.transform(penguin_test)\n",
    "val result = predDF.select(\"label\", \"prediction\", \"probability\")\n",
    "val resutDF = result.withColumnRenamed(\"prediction\", \"Predicted_label\")\n",
    "resutDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that in this case we have obtained the predicted values direclty as integers, and then we don't need to choose a threshold. In addition, in this case we obtain also the probabilities, while in the generalized linear model fitted previously we did not have the probabilities.\n",
    "\n",
    "We compte the same metrics than before, and we observe that we obtain the same results. Thus, the Naive Bayes model is working well, and at the same level than the generalized linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate: 59.09090909090909%\n",
      "False positive rate: 4.545454545454546%\n",
      "True negative rate: 36.36363636363637%\n",
      "False negative rate: 0.0%\n",
      "Matthews correlation coefficient: 0.9085135251589957\n",
      "Classification accuracy: 0.9642857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tVSpDF: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "TC: Long = 22\n",
       "tp: Double = 0.5909090909090909\n",
       "tn: Double = 0.36363636363636365\n",
       "fp: Double = 0.045454545454545456\n",
       "fn: Double = 0.0\n",
       "MCC: Double = 0.9085135251589957\n",
       "accuracy: Double = 0.9642857142857143\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tVSpDF = predDF.select(\"label\", \"prediction\") // True vs predicted labels\n",
    "val TC = predDF.count() \n",
    "\n",
    "val tp = tVSpDF.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() / TC.toDouble\n",
    "val tn = tVSpDF.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count() / TC.toDouble\n",
    "val fp = tVSpDF.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() / TC.toDouble\n",
    "val fn = tVSpDF.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() / TC.toDouble\n",
    "val MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn)) \n",
    "val accuracy = evaluator.evaluate(tVSpDF)\n",
    "\n",
    "\n",
    "println(\"True positive rate: \" + tp *100 + \"%\")\n",
    "println(\"False positive rate: \" + fp * 100 + \"%\")\n",
    "println(\"True negative rate: \" + tn * 100 + \"%\")\n",
    "println(\"False negative rate: \" + fn * 100 + \"%\")\n",
    "println(\"Matthews correlation coefficient: \" + MCC)\n",
    "println(\"Classification accuracy: \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model is not doing the classification perfect, but we are getting nice results also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous variable prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my great hobbies is the mountains. I am from Cardedeu, a village in Vallès Oriental located at the foothills of Montseny. I know perfectly all the paths and trails that surround my town, and I frequent a lot the mountains that form the natural park of Montseny. Due to my interest in mountains and forests I thought it would be interesting to use as a data set one that is related to this. In this part of the work we will use a data set downloaded from the National Forest Inventory of Catalonia. The NFI has a very interesting web where they upload a lot of data related to the natural environment in Catalonia, see the website on https://laboratoriforestal.creaf.cat/nfi_app/. This web allows us a very attractive visualization of the data in map format and also in table format. In addition, it allows us to download in csv format the data we want. What I have done has been to select data on forests in the Vallés Occidental, Vallés Oriental, Osona and Moianès. The objective of this second part of the work is to predict the continuous variable of tree density. In the data set that we have downloaded we have a total of 696 observations. It should be noted that the observations in the data set are from plots (not individual trees). The variables we have are plot ID (unique identifier for each plot), basal area in $m^2/ha$, total aerial biomass in $t/ha$, accumulated aerial CO2 in $t/ha$, dbh (diameter at breast height) in cm, tree density in trees/ha, dominant species in density, county, municipality, municipality id. \n",
    "\n",
    "Our goal for predicting the density of trees, will be fit a linear model firstly, which is one of the simplest but most useful models that we have. And then we will try to fit a generalized linear model using the Gamma distribution. I am interested in study cases where the Gamma distribution works better for modeling, and I think that this part of the work is a good opportunity for compare the normal linear model and the generalized linear model and see which works better.\n",
    "\n",
    "The first step, as always, is import the data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trees: org.apache.spark.sql.DataFrame = [plot_id: string, basal_area: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var trees = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"/Users/arnaugarcia/Desktop/Q1/bases de dades/zeppelin/data/arbres_cat.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print several observations of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+\n",
      "|plot_id|basal_area|aerial_biomass_total|accumulated_aerial_co2|  dbh|density|density_species_dominant|admin_region|admin_municipality|admin_municipality_id|\n",
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+\n",
      "|P_01685|     42.73|               145.4|                272.71|21.86|1138.84|             Pinus nigra|     Moianès|              Moià|                81385|\n",
      "|P_00480|     22.31|               88.69|                165.49|20.35| 686.13|        Pinus sylvestris|       Osona|            Alpens|                80044|\n",
      "|P_00499|     14.64|               57.67|                102.97|17.98| 576.49|         Quercus humilis|       Osona|            Alpens|                80044|\n",
      "|P_00520|     21.33|               97.69|                182.39|21.57| 583.57|        Pinus sylvestris|       Osona|            Alpens|                80044|\n",
      "|P_00521|     17.43|               64.88|                120.17|17.99| 686.13|        Pinus sylvestris|       Osona|            Alpens|                80044|\n",
      "|P_00535|     10.87|               36.08|                 67.31|15.89|  548.2|        Pinus sylvestris|       Osona|             Lluçà|                81094|\n",
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to develop an exploratory analysis, compute some statistics and see if there are missing values in our data set. We will be using similar code than in the first part. First, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+--------------------+----------------------+------------------+-----------------+------------------------+---------------+--------------------+---------------------+\n",
      "|summary|plot_id|        basal_area|aerial_biomass_total|accumulated_aerial_co2|               dbh|          density|density_species_dominant|   admin_region|  admin_municipality|admin_municipality_id|\n",
      "+-------+-------+------------------+--------------------+----------------------+------------------+-----------------+------------------------+---------------+--------------------+---------------------+\n",
      "|  count|    696|               696|                 696|                   696|               696|              696|                     696|            696|                 696|                  696|\n",
      "|   mean|   NULL|18.688433908045976|   87.34110632183904|    157.70793103448278|18.639727011494255|832.2389655172417|                    NULL|           NULL|                NULL|     87384.6393678161|\n",
      "| stddev|   NULL| 9.877297419153532|  59.170255531285314|    106.96388027499285| 6.162784553312723|591.2291099467486|                    NULL|           NULL|                NULL|    21473.42759799767|\n",
      "|    min|P_00480|              0.45|                1.06|                  1.97|              7.63|             5.09|          Acer campestre|        Moianès|          Aiguafreda|                80044|\n",
      "|    max|P_08456|             56.93|               459.2|                859.83|             60.25|          3388.23|             Ulmus minor|Vallès Oriental|les Masies de Vol...|               172207|\n",
      "+-------+-------+------------------+--------------------+----------------------+------------------+-----------------+------------------------+---------------+--------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summaryStats: org.apache.spark.sql.DataFrame = [summary: string, plot_id: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summaryStats = trees.describe()\n",
    "summaryStats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the density variable, which is the one of most interest because is the one that we will try to predict, we see that the minimum and the maximum seems to be very extreme values, in the sense that are values very far from the mean. This gives us an idea that the predictions we are going to make are not simple, in the sense that there is a very large range of values of the density variable and therefore many different values to predict.\n",
    "\n",
    "Now we count the distinct values for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "|plot_id|basal_area|aerial_biomass_total|accumulated_aerial_co2|dbh|density|density_species_dominant|admin_region|admin_municipality|admin_municipality_id|\n",
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "|    696|       635|                 687|                   688|573|    548|                      34|           4|               102|                  102|\n",
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees.select(trees.columns.map(plot_id => countDistinct(col(plot_id)).alias(plot_id)): _*).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we were expecting, we obtain $696$ different values for the variable plot_id, which is exacltly the number of observations (remember that the plot_id is a unique id for each plot). The different values for the admin_region is $4$, which is also what we were expecting because we have selected only four catalan \"comarques\".\n",
    "\n",
    "In the following chunk code we compute correlations between several variables. In this case we are hugely interested in how behaves the density variable with respect other continuous variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between density and basal area: 0.6361732127094407\n",
      "Correlation between density and aerial biomass: 0.4122166574338511\n",
      "Correlation between density and dbh: -0.5200283339124606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correlation1: Double = 0.6361732127094407\n",
       "correlation2: Double = 0.4122166574338511\n",
       "correlation3: Double = -0.5200283339124606\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val correlation1 = trees.stat.corr(\"density\", \"basal_area\")\n",
    "val correlation2 = trees.stat.corr(\"density\", \"aerial_biomass_total\")\n",
    "val correlation3 = trees.stat.corr(\"density\", \"dbh\")\n",
    "\n",
    "println(s\"Correlation between density and basal area: $correlation1\")\n",
    "println(s\"Correlation between density and aerial biomass: $correlation2\")\n",
    "println(s\"Correlation between density and dbh: $correlation3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us interpret the correlations obtained:\n",
    "- The correlation between density and basal area is $0.64$, which indicates that the greater basal area the greater the density of trees. This makes sense, the basal area is computed doing the sum of cross sectional area of tree trunks, divided by the surface area of the plot. Then, the more basal area, the more the sum of the cross sectional area of tree trunks (obiously the surface area does not change), which may indicate more trees, and then more density, or thicker trees.\n",
    "- The correlation between aerial biomass and density is $0.41$, which indicates that the greater the aerial biomass the greater the density.The arial biomass is the sum of total amount of organic matter from leaves, branches, trunk and bark. Thus, makes sense that the more organic matter, the more trees and then more density.\n",
    "- The correlation between density and dbh is $-0.52$, which indicates that the lower the dbh, the lower the density. It makes a lot of sense, because the dbh is the diameter at the breast height of the tree, then if the dbh is big, there is less space for other trees and then there is less density of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step, is check if there are missing values in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "|plot_id|basal_area|aerial_biomass_total|accumulated_aerial_co2|dbh|density|density_species_dominant|admin_region|admin_municipality|admin_municipality_id|\n",
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "|      0|         0|                   0|                     0|  0|      0|                       0|           0|                 0|                    0|\n",
      "+-------+----------+--------------------+----------------------+---+-------+------------------------+------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees.select(trees.columns.map(colName => sum(when(col(colName).isNull, 1).otherwise(0)).alias(colName)): _*).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing values, we are fortunate that the NFI databases are of good quality. Then, we can keep with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk we use the feature engineering, as we did in the previous section. This procedure is always mandatory if one wants to fit models using spark. We use the method `VectorAssembler` as we did in the previoius section, and we include in the correspondent array the independent variables of our model (it is, the variables with respect we will be modeling the density of trees). \n",
    "\n",
    "When we try to include in our vector assembler variables that are not numerical, ie variable of string type, we obtain an error. Then, what we should do is convert these variables into numerical. For this purpose, the `StringIndexer` method used in the previous section will be useful. Then, in the following chunk we convert to numerical the categorical variables putting indexes.\n",
    "\n",
    "We will use the useful pipeline in order to use at the same time the `StringIndexer` method and the `VectorAssembler` method. We define use twice the `StringIndexer` method, for convert the categorical variables density_species_dominant and admin_region into categorical but numerical variables. Notice that we have the admin_municipality_id, which is a numerical id of the different municipalities, and then we don't need to transform this categorical variable. Then, with a pipeline we gather the different operations to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "labelindexer1: org.apache.spark.ml.feature.StringIndexer = strIdx_3d7d902674ff\n",
       "labelindexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_8d7e11b183a4\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_91806c22242d, handleInvalid=error, numInputCols=7\n",
       "pipeline_trees: org.apache.spark.ml.Pipeline = pipeline_a0fe77d6e63d\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "val labelindexer1 = new StringIndexer()\n",
    "      .setInputCol(\"density_species_dominant\")\n",
    "      .setOutputCol(\"species_index\")\n",
    "val labelindexer2 = new StringIndexer()\n",
    "      .setInputCol(\"admin_region\")\n",
    "      .setOutputCol(\"comarca_index\")\n",
    "val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"basal_area\", \"aerial_biomass_total\", \"accumulated_aerial_co2\", \"dbh\", \"species_index\", \"comarca_index\", \"admin_municipality_id\"))\n",
    "      .setOutputCol(\"features\")\n",
    "val pipeline_trees = new Pipeline()\n",
    "      .setStages(Array(labelindexer1, labelindexer2, assembler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+-------------+-------------+--------------------+\n",
      "|plot_id|basal_area|aerial_biomass_total|accumulated_aerial_co2|  dbh|density|density_species_dominant|admin_region|admin_municipality|admin_municipality_id|species_index|comarca_index|            features|\n",
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+-------------+-------------+--------------------+\n",
      "|P_01685|     42.73|               145.4|                272.71|21.86|1138.84|             Pinus nigra|     Moianès|              Moià|                81385|          8.0|          2.0|[42.73,145.4,272....|\n",
      "|P_00480|     22.31|               88.69|                165.49|20.35| 686.13|        Pinus sylvestris|       Osona|            Alpens|                80044|          2.0|          0.0|[22.31,88.69,165....|\n",
      "|P_00499|     14.64|               57.67|                102.97|17.98| 576.49|         Quercus humilis|       Osona|            Alpens|                80044|          1.0|          0.0|[14.64,57.67,102....|\n",
      "|P_00520|     21.33|               97.69|                182.39|21.57| 583.57|        Pinus sylvestris|       Osona|            Alpens|                80044|          2.0|          0.0|[21.33,97.69,182....|\n",
      "|P_00521|     17.43|               64.88|                120.17|17.99| 686.13|        Pinus sylvestris|       Osona|            Alpens|                80044|          2.0|          0.0|[17.43,64.88,120....|\n",
      "+-------+----------+--------------------+----------------------+-----+-------+------------------------+------------+------------------+---------------------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trees_format: org.apache.spark.sql.DataFrame = [plot_id: string, basal_area: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var trees_format = pipeline_trees.fit(trees).transform(trees)\n",
    "trees_format.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with the spark methods we need only two columns, one named label, with the response variable (density in our case), and other column with the features created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|  label|            features|\n",
      "+-------+--------------------+\n",
      "|1138.84|[42.73,145.4,272....|\n",
      "| 686.13|[22.31,88.69,165....|\n",
      "| 576.49|[14.64,57.67,102....|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trees_format: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees_format = trees_format.select($\"density\".alias(\"label\"), $\"features\")\n",
    "trees_format.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is split the data into a test and train dsta sets. First we create an instance for random split de data, and then we divide the data set using the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splits = trees_format.randomSplit(Array(0.7, 0.3), seed = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 5.09|[1.45,10.16,17.92...|\n",
      "|10.19|[2.44,15.16,27.32...|\n",
      "|14.15|[0.9,3.51,6.23,28...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trees_train: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trees_train = splits(0).toDF\n",
    "trees_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|14.15|[1.02,3.27,6.15,3...|\n",
      "|28.29|[2.34,9.62,17.06,...|\n",
      "|31.83|[0.94,3.24,5.68,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trees_test: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trees_test = splits(1).toDF\n",
    "trees_test.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to create a normal linear model and to fit this model to our train data set. We want to use the normal linear model: $density=\\beta_0 + \\beta_1 basal-area + \\beta_2 aerial-biomass-total + \\beta_3 accumulated-aerial-co2 + \\beta_4 dbh + \\beta_5 species-index + \\beta_6 comarca-index + \\beta_7 admin- municipality-id$.\n",
    "\n",
    " We import the necessary items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegressionModel\n",
       "import org.apache.spark.ml.regression.LinearRegression\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegressionModel\n",
    "import org.apache.spark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a normal linear model estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_593277aa7317\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LinearRegression()\n",
    "            .setMaxIter(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we fit the model with the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_model: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_593277aa7317, numFeatures=7\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linear_model = lr.fit(trees_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients and the intercept of our normal linear model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 436.1380936496392\n",
      "Coefficients: [79.67746000024212,68.90600838677362,-42.21008818547648,-30.2181412264526,-1.2128290989741863,16.61569298529879,0.001271400606837391]\n"
     ]
    }
   ],
   "source": [
    "println(s\"Intercept: ${linear_model.intercept}\")\n",
    "println(s\"Coefficients: ${linear_model.coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute two important metrics for assess thr quality of our model, the MAE (Mean Absolute Error) and RMSE(Root Mean Squared Error) over the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 184.30215348400958\n",
      "RMSE: 264.62229044835624\n"
     ]
    }
   ],
   "source": [
    "println(s\"MAE: ${linear_model.summary.meanAbsoluteError}\")\n",
    "println(s\"RMSE: ${linear_model.summary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to see if our model is woking good we apply the fitted model to the test data set, and we compare the predicter values obtained with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "| label|            features|         prediction|\n",
      "+------+--------------------+-------------------+\n",
      "| 14.15|[1.02,3.27,6.15,3...| -287.8088538577296|\n",
      "| 28.29|[2.34,9.62,17.06,...| -278.5035563341956|\n",
      "| 31.83|[0.94,3.24,5.68,1...|  45.00945689438515|\n",
      "| 51.07|[2.15,10.47,19.26...|  -60.0618515837682|\n",
      "| 61.68|[4.12,13.53,25.13...| -93.56367616458823|\n",
      "| 84.88|[7.13,48.62,84.81...|-58.103598085878616|\n",
      "|105.68|[5.57,27.2,49.76,...|-24.198375826501092|\n",
      "|109.64|[2.26,7.39,13.75,...|  157.0179698908825|\n",
      "|109.64|[2.81,14.11,24.68...| 165.15342718236633|\n",
      "|119.83|[5.13,28.34,51.98...|-11.262788988199077|\n",
      "|127.32|[0.62,1.3,2.28,7....|  367.3287852698767|\n",
      "|127.32|[0.68,1.15,2.15,8...|  380.2689991386181|\n",
      "|135.95|[10.49,49.61,91.4...| 15.027471146861842|\n",
      "|152.08|[5.55,15.02,28.04...| 191.86251262210743|\n",
      "|159.15|[1.26,3.95,7.12,1...|  360.2031483966067|\n",
      "| 173.3|[1.75,6.64,11.63,...| 319.19352606987536|\n",
      "|179.95|[6.45,24.96,46.53...| 188.24452211132697|\n",
      "|189.01|[8.68,46.01,83.49...| 195.64882942157442|\n",
      "|195.51|[22.24,190.92,337...|  75.68021218158441|\n",
      "|198.06|[6.01,23.01,42.15...|  259.3325837449737|\n",
      "|204.71|[10.4,56.76,102.5...| 234.29389775132015|\n",
      "|204.71|[13.5,88.99,157.4...| 196.12996525149686|\n",
      "|211.78|[9.24,43.43,76.83...| 271.61301627824173|\n",
      "|229.89|[9.55,44.92,79.4,...|  349.2803375131356|\n",
      "|236.96|[4.01,13.55,24.89...|  329.7561395382592|\n",
      "| 238.1|[13.81,57.88,106....| 344.33867218767216|\n",
      "|247.57|[7.73,25.55,47.46...|  338.5002470272436|\n",
      "|250.69|[10.9,36.62,68.57...|  376.9799898973318|\n",
      "|270.35|[7.75,42.11,74.9,...|  335.1948399760164|\n",
      "| 272.9|[26.84,216.83,383...|  392.1376256844703|\n",
      "|282.94|[7.92,34.02,60.24...| 400.53842628905034|\n",
      "|298.65|[10.28,40.94,74.1...|   417.939596806598|\n",
      "|300.48|[20.66,122.09,222...|  345.6584517081412|\n",
      "|318.31|[5.6,21.38,37.52,...|  455.6156823800809|\n",
      "|319.44|[15.16,71.46,129....|  447.5008890561664|\n",
      "|324.96|[5.93,29.65,53.78...|  373.3220068294725|\n",
      "|328.92|[6.59,26.64,46.98...| 451.85002088119523|\n",
      "|335.99|[11.39,38.0,70.45...| 494.89649855330765|\n",
      "|347.03|[45.83,365.18,670...| -81.53105112642169|\n",
      "|357.21|[16.12,68.17,126....| 486.38896535496167|\n",
      "+------+--------------------+-------------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "test: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "val test = linear_model.transform(trees_test)\n",
    "test.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems thar there are bad predictions, we obtain different negative predictions! Which have no sense. On the other hand, it seems that there are a lot of values quite well predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[254] at map at <console>:49\n",
       "res35: Double = 45.00945689438515\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = test.select(\"prediction\").rdd.map(_.getDouble(0))\n",
    "predictions.collect()(0)\n",
    "predictions.collect()(1)\n",
    "predictions.collect()(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[275] at map at <console>:49\n",
       "res38: Double = 31.83\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labels = test.select(\"label\").rdd.map(_.getDouble(0))\n",
    "labels.collect()(0)\n",
    "labels.collect()(1)\n",
    "labels.collect()(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|                 _1|    _2|\n",
      "+-------------------+------+\n",
      "| -287.8088538577296| 14.15|\n",
      "| -278.5035563341956| 28.29|\n",
      "|  45.00945689438515| 31.83|\n",
      "|  -60.0618515837682| 51.07|\n",
      "| -93.56367616458823| 61.68|\n",
      "|-58.103598085878616| 84.88|\n",
      "|-24.198375826501092|105.68|\n",
      "+-------------------+------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.zip(labels).toDF.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now the MAE and the RMSE for the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 198.62821943451837\n",
      "RMSE: 265.5134459147396\n"
     ]
    }
   ],
   "source": [
    "println(s\"MAE: ${new RegressionMetrics(predictions.zip(labels)).meanAbsoluteError}\")\n",
    "println(s\"RMSE: ${new RegressionMetrics(predictions.zip(labels)).rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have:\n",
    "- Train data set: $MAE = 184.30215348400958$ and $RMSE=264.62229044835624$.\n",
    "- Test data set: $MAE = 198.62821943451837$ and $RMSE=265.5134459147396$.\n",
    "\n",
    "We can observe that we are obtaining very similar quantity, which is a good indication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit a generalized linear model using the Gamma distribution and the logarithm as a link function. We start creating an estimator of this glm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma: org.apache.spark.ml.regression.GeneralizedLinearRegression = glm_eb557c9c3874\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gamma = new GeneralizedLinearRegression()\n",
    "            .setFamily(\"gamma\") \n",
    "            .setLink(\"log\")\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model with the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma_model: org.apache.spark.ml.regression.GeneralizedLinearRegressionModel = GeneralizedLinearRegressionModel: uid=glm_eb557c9c3874, family=gamma, link=log, numFeatures=7\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gamma_model = gamma.fit(trees_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the spark documentation (see https://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression) we have found that we can compute the following quantities related with the fitted model that are key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Standard Errors: 0.0035617171613868213,0.005835937380874954,0.003337727323074195,0.0023225515860912185,0.0022646874176271595,0.01116777140704118,5.768325951989856E-7,0.07487153599031958\n",
      "T Values: 21.340519100129626,1.6091380258261347,-1.9366082777667895,-37.68955192642184,-2.5703181894336073,-0.7661312030136829,0.4168110627069901,91.9416285111697\n",
      "P Values: 0.0,0.10824089180329333,0.053377372587454675,0.0,0.0104597281203902,0.44397326489535405,0.6770021704955014,0.0\n",
      "Dispersion: 0.06650579221999973\n",
      "Null Deviance: 317.2640840876833\n",
      "Residual Degree Of Freedom Null: 489\n",
      "Deviance: 45.62673842741864\n",
      "Residual Degree Of Freedom: 482\n",
      "AIC: 6529.7029807387335\n",
      "Deviance Residuals: \n",
      "+--------------------+\n",
      "|   devianceResiduals|\n",
      "+--------------------+\n",
      "|-0.10594256040829246|\n",
      "| 0.27291385727787965|\n",
      "| -1.3880137980016374|\n",
      "| -1.2988922107182197|\n",
      "| -0.5049597804949391|\n",
      "| -1.1070951774037816|\n",
      "|  -1.008568076141114|\n",
      "| -0.3574987988977449|\n",
      "| -1.6557142388872728|\n",
      "|  -1.545992593853271|\n",
      "| -1.4993919355725527|\n",
      "| -1.4664576860933733|\n",
      "|  -0.538423818319976|\n",
      "| -0.4571537823254411|\n",
      "| -1.0126835531177691|\n",
      "| -0.7159472902353702|\n",
      "|  -1.145964024156902|\n",
      "|  0.5209147191801561|\n",
      "|-0.01349867274058...|\n",
      "| -0.8368457734836718|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary: org.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary =\n",
       "Coefficients:\n",
       "             Feature Estimate Std Error  T Value P Value\n",
       "         (Intercept)   6.8838    0.0749  91.9416  0.0000\n",
       "          basal_area   0.0760    0.0036  21.3405  0.0000\n",
       "aerial_biomass_total   0.0094    0.0058   1.6091  0.1082\n",
       "accumulated_aeria...  -0.0065    0.0033  -1.9366  0.0534\n",
       "                 dbh  -0.0875    0.0023 -37.6896  0.0000\n",
       "       species_index  -0.0058    0.0023  -2.5703  0.0105\n",
       "       comarca_index  -0.0086    0.0112  -0.7661  0.4440\n",
       "admin_municipalit...   0.0000    0.0000   0.4168  0.6770\n",
       "\n",
       "(Dispersion parameter for gamma family taken to be 0.0665)\n",
       "   Null deviance: 317.2641 on 482 degrees of freedom\n",
       "Residual deviance: 45.6267 on 482 degrees of freedom\n",
       "AIC: 6529.7030\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summary = gamma_model.summary\n",
    "println(s\"Coefficient Standard Errors: ${summary.coefficientStandardErrors.mkString(\",\")}\")\n",
    "println(s\"T Values: ${summary.tValues.mkString(\",\")}\")\n",
    "println(s\"P Values: ${summary.pValues.mkString(\",\")}\")\n",
    "println(s\"Dispersion: ${summary.dispersion}\")\n",
    "println(s\"Null Deviance: ${summary.nullDeviance}\")\n",
    "println(s\"Residual Degree Of Freedom Null: ${summary.residualDegreeOfFreedomNull}\")\n",
    "println(s\"Deviance: ${summary.deviance}\")\n",
    "println(s\"Residual Degree Of Freedom: ${summary.residualDegreeOfFreedom}\")\n",
    "println(s\"AIC: ${summary.aic}\")\n",
    "println(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in the previous output thay almost all the variables are statistically significant (pvalue less than $0,05$). We can observe other key quantities as the AIC, the model parameter estimation, etc.\n",
    "\n",
    "Now, we apply the model to the test data and we see how are the predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+\n",
      "| label|            features|        prediction|\n",
      "+------+--------------------+------------------+\n",
      "| 14.15|[1.02,3.27,6.15,3...| 70.32148138626481|\n",
      "| 28.29|[2.34,9.62,17.06,...| 66.50290470490945|\n",
      "| 31.83|[0.94,3.24,5.68,1...|191.18264932193597|\n",
      "| 51.07|[2.15,10.47,19.26...| 140.0210126492307|\n",
      "| 61.68|[4.12,13.53,25.13...|  98.0861011068805|\n",
      "| 84.88|[7.13,48.62,84.81...| 87.07983175943683|\n",
      "|105.68|[5.57,27.2,49.76,...| 147.3422740498355|\n",
      "|109.64|[2.26,7.39,13.75,...| 277.7888641418412|\n",
      "|109.64|[2.81,14.11,24.68...| 245.0227338565646|\n",
      "|119.83|[5.13,28.34,51.98...|153.97179476382135|\n",
      "|127.32|[0.62,1.3,2.28,7....| 488.4773303714757|\n",
      "|127.32|[0.68,1.15,2.15,8...|486.32064862724854|\n",
      "|135.95|[10.49,49.61,91.4...|121.08474346719352|\n",
      "|152.08|[5.55,15.02,28.04...| 211.5428442679996|\n",
      "|159.15|[1.26,3.95,7.12,1...| 440.0582092626804|\n",
      "| 173.3|[1.75,6.64,11.63,...| 412.8557085151365|\n",
      "|179.95|[6.45,24.96,46.53...|221.12802561786668|\n",
      "|189.01|[8.68,46.01,83.49...|201.84001739369108|\n",
      "|195.51|[22.24,190.92,337...| 118.2755082414303|\n",
      "|198.06|[6.01,23.01,42.15...|256.95923680484566|\n",
      "|204.71|[10.4,56.76,102.5...|202.73118624502962|\n",
      "|204.71|[13.5,88.99,157.4...|159.32125399343275|\n",
      "|211.78|[9.24,43.43,76.83...| 192.7357631223556|\n",
      "|229.89|[9.55,44.92,79.4,...|249.37259223201843|\n",
      "|236.96|[4.01,13.55,24.89...| 352.6921710155192|\n",
      "| 238.1|[13.81,57.88,106....|217.85054064083766|\n",
      "|247.57|[7.73,25.55,47.46...| 282.5334331255303|\n",
      "|250.69|[10.9,36.62,68.57...|256.55816523595274|\n",
      "|270.35|[7.75,42.11,74.9,...| 305.8156537180321|\n",
      "| 272.9|[26.84,216.83,383...|214.47292135559684|\n",
      "|282.94|[7.92,34.02,60.24...| 322.7872490605809|\n",
      "|298.65|[10.28,40.94,74.1...| 314.9108149924939|\n",
      "|300.48|[20.66,122.09,222...| 266.8434849495317|\n",
      "|318.31|[5.6,21.38,37.52,...|387.47533435210863|\n",
      "|319.44|[15.16,71.46,129....| 276.5101783932404|\n",
      "|324.96|[5.93,29.65,53.78...|374.27770227449736|\n",
      "|328.92|[6.59,26.64,46.98...| 381.2856420318415|\n",
      "|335.99|[11.39,38.0,70.45...|339.83582796531334|\n",
      "|347.03|[45.83,365.18,670...| 345.4722622862983|\n",
      "|357.21|[16.12,68.17,126....| 338.2946230689163|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_gamma: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_gamma = gamma_model.transform(trees_test)\n",
    "test_gamma.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this case we don't obtain negative predictions. Thus, this is an argument in favour of the gamma model versus the normal linear model. In addition, we can observe that the predictions are quite good, and seems to be better than the predictions of the normal linear model in general.\n",
    "\n",
    "In order to compare the models we compute the RMSE and the MAE of the test data set as we did before, and compare the values obtained for the gamma model and for the normal linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[369] at map at <console>:52\n",
       "res47: Double = 191.18264932193597\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = test_gamma.select(\"prediction\").rdd.map(_.getDouble(0))\n",
    "predictions.collect()(0)\n",
    "predictions.collect()(1)\n",
    "predictions.collect()(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[376] at map at <console>:52\n",
       "res48: Double = 31.83\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labels = test_gamma.select(\"label\").rdd.map(_.getDouble(0))\n",
    "labels.collect()(0)\n",
    "labels.collect()(1)\n",
    "labels.collect()(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|                _1|    _2|\n",
      "+------------------+------+\n",
      "| 70.32148138626481| 14.15|\n",
      "| 66.50290470490945| 28.29|\n",
      "|191.18264932193597| 31.83|\n",
      "| 140.0210126492307| 51.07|\n",
      "|  98.0861011068805| 61.68|\n",
      "| 87.07983175943683| 84.88|\n",
      "| 147.3422740498355|105.68|\n",
      "| 277.7888641418412|109.64|\n",
      "| 245.0227338565646|109.64|\n",
      "|153.97179476382135|119.83|\n",
      "| 488.4773303714757|127.32|\n",
      "|486.32064862724854|127.32|\n",
      "|121.08474346719352|135.95|\n",
      "| 211.5428442679996|152.08|\n",
      "| 440.0582092626804|159.15|\n",
      "| 412.8557085151365| 173.3|\n",
      "|221.12802561786668|179.95|\n",
      "|201.84001739369108|189.01|\n",
      "| 118.2755082414303|195.51|\n",
      "|256.95923680484566|198.06|\n",
      "+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.zip(labels).toDF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 165.03570518042602\n",
      "RMSE: 324.3085890531792\n"
     ]
    }
   ],
   "source": [
    "println(s\"MAE: ${new RegressionMetrics(predictions.zip(labels)).meanAbsoluteError}\")\n",
    "println(s\"RMSE: ${new RegressionMetrics(predictions.zip(labels)).rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the quantities obtained with the ones obtained in the case of the normal linear model, we can see that we have obtained less MAE, indicating that the gamma model is better. But, on the other hand, we have obtained a higher RMSE, indicating that the normal linear model is better.\n",
    "\n",
    "Thus, it is not easy to decide which of the two models is better. Notwithstanding, looking at the results obtained and above all, taking into account that the gamma model does not return negative values, I think I would choose the gamma model with link function the logarithm before the normal linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Discussion_: Here, you would then have to chose which model works best and actually implement it into a certain problem. Just argue which one (for both examples) you would use and why (and why not the others?).\n",
    "\n",
    "In the case of the prediction of the categorical variable, the two models used (Naive Bayes and binomial glm) classify well. However, I think that in this case, i.e., to solve the problem of classifying penguin species given the dataset we have, I would stick with Binomial GLM with threshold $0.5$, becaus the model is classifying the penguins with no fail. On the other hand Naive Bayes is interesting because this model gives us not only the predictions but also the probabilities, which gives us extra information that the glm binomial model does not give us. \n",
    "\n",
    "For the models that we have used to predict a continuous variable, I think I would choose the gamma model because of what I said before, the fact that the normal linear model gives negative predictions makes me choose the gamma model. Well, despite the fact that the models behave similarly in terms of MAE and RMSE metrics, predicting tree density values in a forest with negative values is quite serious. On the other hand, I think that is interesing to study the normal linear model but with transformations in the variables. It is usual to model with normal linear models with transformations, and doing the right transformations can give us good results. Thus, compare the normal linear model with the Gamma regression model adding transformations would be an interesting topic for future research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
